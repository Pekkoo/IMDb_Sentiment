{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment analysis\n",
    "In this project, IMDB movie review texts by users are classified as either negative or positive. Labels for sentiment exist in the training set, so this is a supervised learning problem. In the accompanying Kaggle competition, evaluation metric is Area Under Curve (AUC), so predictions are probabilities of reviews being positive. There is a \"leak\" in the test set which allows us to see the ground truth and therefore AUC scores without submitting to Kaggle. \n",
    "\n",
    "First, simple Bag of Words + logistic regression approach is evaluated, which achieves surprisingly good results on the test set. Then, a convolutional neural network (CNN) is trained for the same purpose. It does not perform as well with the current architecture but by combining the predictions with the simpler model, we get a decent boost.\n",
    "\n",
    "**Note**: this is a work in progress. Documentation will improve soon. Also pretrained word vectors and different neural network architectures will be examined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "DATA_DIR = 'D:/Data/Kaggle_imdb/'\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labeled = pd.read_csv(DATA_DIR + 'labeledTrainData.tsv', sep='\\t', quoting=3)\n",
    "train_unlabeled = pd.read_csv(DATA_DIR + 'unlabeledTrainData.tsv', sep='\\t', quoting=3)\n",
    "test = pd.read_csv(DATA_DIR + 'testData.tsv', sep='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print train_labeled['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "print train_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'\n",
      " '\"\\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"']\n"
     ]
    }
   ],
   "source": [
    "print train_labeled['review'].values[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text preprocessing several techniques, such as lemmatizing and removing stop words, were tried. CV score was found to be the highest without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(review):\n",
    "    text = BeautifulSoup(review, 'html5lib').get_text().lower()\n",
    "    text = re.sub('[^A-Za-z0-9! ]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_reviews = train_labeled['review'].apply(text_process).values\n",
    "test_reviews = test['review'].apply(text_process).values\n",
    "unlabeled_reviews = train_unlabeled['review'].apply(text_process).values\n",
    "train_sentiment = train_labeled['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentiment = test['id'].apply(lambda x: (int(x.split('_')[1][:-1]) > 5) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf decreased the cross-validated AUC score so it was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_pipe = Pipeline([\n",
    "    ('bow', CountVectorizer(ngram_range=(1,2), min_df=2)),\n",
    "    #('tf-idf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean: 0.955424096, CV std: 0.00338045430448\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_score = cross_val_score(text_pipe, train_reviews, train_sentiment, \n",
    "                           cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "print 'CV mean: {}, CV std: {}'.format(cv_score.mean(), cv_score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        str...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipe.fit(train_reviews, train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9570968768\n"
     ]
    }
   ],
   "source": [
    "lr_preds = text_pipe.predict_proba(test_reviews)[:,1]\n",
    "print roc_auc_score(test_sentiment, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE MUST GO DEEPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascii_train = [review.encode('ascii') for review in train_reviews]\n",
    "ascii_test = [review.encode('ascii') for review in test_reviews]\n",
    "ascii_unlabeled = [review.encode('ascii') for review in unlabeled_reviews]\n",
    "ascii_all = ascii_train + ascii_test + ascii_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_labeled['review'].apply(lambda x: len(x.split())), 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 8000\n",
    "SEQ_LEN = 800\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(ascii_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(ascii_train)\n",
    "test_seq = tokenizer.texts_to_sequences(ascii_test)\n",
    "unlabeled_seq = tokenizer.texts_to_sequences(ascii_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = sequence.pad_sequences(train_seq, maxlen=SEQ_LEN)\n",
    "test_seq = sequence.pad_sequences(test_seq, maxlen=SEQ_LEN)\n",
    "unlabeled_seq = sequence.pad_sequences(unlabeled_seq, maxlen=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(train_seq)) < 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_seq[mask]\n",
    "y_train = train_sentiment[mask]\n",
    "X_val = train_seq[~mask]\n",
    "y_val = train_sentiment[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2549 samples\n",
      "Epoch 1/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.6942 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5347\n",
      "Epoch 2/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.6893 - acc: 0.5305 - val_loss: 0.6859 - val_acc: 0.5685\n",
      "Epoch 3/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.5838 - acc: 0.7124 - val_loss: 0.4464 - val_acc: 0.8246\n",
      "Epoch 4/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.3574 - acc: 0.8607 - val_loss: 0.3303 - val_acc: 0.8686\n",
      "Epoch 5/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.2680 - acc: 0.9000 - val_loss: 0.2995 - val_acc: 0.8729\n",
      "Epoch 6/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.2158 - acc: 0.9249 - val_loss: 0.2859 - val_acc: 0.8803\n",
      "Epoch 7/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.1782 - acc: 0.9401 - val_loss: 0.2764 - val_acc: 0.8843\n",
      "Epoch 8/8\n",
      "22451/22451 [==============================] - 1s - loss: 0.1491 - acc: 0.9541 - val_loss: 0.2919 - val_acc: 0.8796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e6c24710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(MAX_WORDS, 32, input_length=SEQ_LEN),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(Adam(lr=0.0001), 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=8, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2549 samples\n",
      "Epoch 1/4\n",
      "22451/22451 [==============================] - 5s - loss: 0.6930 - acc: 0.5069 - val_loss: 0.6916 - val_acc: 0.5253\n",
      "Epoch 2/4\n",
      "22451/22451 [==============================] - 5s - loss: 0.6230 - acc: 0.6683 - val_loss: 0.4808 - val_acc: 0.8027\n",
      "Epoch 3/4\n",
      "22451/22451 [==============================] - 5s - loss: 0.4086 - acc: 0.8268 - val_loss: 0.3359 - val_acc: 0.8627\n",
      "Epoch 4/4\n",
      "22451/22451 [==============================] - 5s - loss: 0.3200 - acc: 0.8719 - val_loss: 0.2951 - val_acc: 0.8811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa0c3ff748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = Sequential([\n",
    "    Embedding(MAX_WORDS, 50, input_length=SEQ_LEN),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "conv_model.compile(RMSprop(lr=0.0001), 'binary_crossentropy', metrics=['accuracy'])\n",
    "conv_model.fit(X_train, y_train, epochs=4, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2549 samples\n",
      "Epoch 1/2\n",
      "22451/22451 [==============================] - 5s - loss: 0.2803 - acc: 0.8901 - val_loss: 0.2752 - val_acc: 0.8925\n",
      "Epoch 2/2\n",
      "22451/22451 [==============================] - 5s - loss: 0.2545 - acc: 0.9025 - val_loss: 0.2659 - val_acc: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa0bdd3978>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.optimizer.lr = 0.00001\n",
    "conv_model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights('val_acc_8917.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.load_weights('val_acc_8917.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2549 samples\n",
      "Epoch 1/1\n",
      "22451/22451 [==============================] - 5s - loss: 0.2348 - acc: 0.9121 - val_loss: 0.2626 - val_acc: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa0bdd3cc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_pred = conv_model.predict(test_seq, batch_size=BATCH_SIZE * 2).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN does not achieve as good performance as the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9546203456\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(test_sentiment, cnn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.93157591],\n",
       "       [ 0.93157591,  1.        ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([lr_preds, cnn_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions by the CNN are still different enough compared to the simpler model to provide a decent boost when combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9612864256\n"
     ]
    }
   ],
   "source": [
    "combined_preds = (cnn_pred * 4 + lr_preds * 6) / 2\n",
    "print roc_auc_score(test_sentiment, combined_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the AUC score of 0.96129 we would be at position 65 out of 578 in the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'id': test.index, 'sentiment': combined_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('submission.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
